1. Which type of information is not used for evaluation in machine translation (e.g. in BLEU score), but is used for evaluation in simplification task (e.g. in SARI score)?


System output  

Human reference 

System input (chosen)
======================================

2. Let us consider a simplification task and denote an input by II, a reference by RR, and a system output by OO. We have discussed several types of operations for simplification. How would you compute a precision score for the copying operation?

|O n I n R| / |O n I|

=======================================

3. In the summarization video we talked about attention distribution and denoted its elements as p_{i}^jp 
i
j
​	 . How are they normalized?


They sum to 1 over all positions of an input sentence: \sum_{i} p_i^j = 1∑ 
i
​	 p 
i
j
​	 =1.


They sum to 1 over all positions of an output sentence: \sum_{j} p_i^j = 1∑ 
j
​	 p 
i
j
​	 =1.


They are logits, we need to apply softmax to get the normalization constraint. (chosen)
=====================================

Imagine you have trained an encoder-decoder-attention model to generate a text summary. Let's say you have a vocabulary [big, black, bug, bear] and the vocabulary distribution at some decoding moment is [0.3, 0.4, 0.1, 0.2].

Now, let us consider how it changes if we add the pointer part from the paper "Get to the point! Summarization with pointer-generator network" to be able to copy some input words.

Consider an input sentence: "a big black bug bit a big black bear". And the attention distribution [0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1].

How will the final distribution look like, if the pointer network (copy distribution) is weighted equally with the generator network (vocabulary distribution)?

Enter the probability for "black".


big = 0.3
black = 0.4
bug = 0.1
bear = 0.2

a big black bug bit a big black bear
a = 0.1
big = 0.1
black = 0.1
bug = 0.1
bit = 0.2
a = 0.1
big = 0.1
black = 0.1
bear = 0.1


a = 2 / 0.1
big = 2 / 0.1
black = 2 / 0.1 = 0.2
bug = 0.1
bit = 0.2
bear = 0.1

= 0.5 * 0.4  + 0.5 * 0.2 = 0.2+0.1 = 0.3

The generator part gives you 0.4, the copy part gives you 0.2. So the final weighted probability is 0.5 * 0.4 + 0.5 * 0.2 = 0.3.

======================================


Check the correct statements about the summarization models discussed in the video.


The pointer-generator network performs extractive summarization. 


The coverage trick helps to avoid repetitions of the input fragments. (chosen)


The copy mechanism encourages the model to generate new fragments that did not occur in the input.


The pointer-generator network with coverage trick outperforms all other baselines.


The pointer-generator network performs abstractive summarization. (chosen)

​	



