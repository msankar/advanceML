Here's a list of papers mentioned in week 5 slides.

Video 1:

A. Bhargava, A. Celikyilmaz, D. Hakkani-Tur, and R. Sarikaya. EASY CONTEXTUAL INTENT PREDICTION AND SLOT DETECTION (2013). http://www.cs.toronto.edu/~aditya/publications/contextual.pdf
K. Scheffler and S. Young. Simulation of human-machine dialogues (1999). http://mi.eng.cam.ac.uk/~sjy/papers/scyo99.ps.gz
Video 2:

Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Sch√ºtze. Comparative Study of CNN and RNN for Natural Language Processing (2017). https://arxiv.org/pdf/1702.01923.pdf
Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier. Language Modeling with Gated Convolutional Networks (2017). https://arxiv.org/pdf/1612.08083.pdf
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin. Convolutional Sequence to Sequence Learning (2017). https://arxiv.org/pdf/1705.03122.pdf
Bing Liu, Ian Lane. Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling (2016). https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1352.PDF
Gokhan Tur, Dilek Hakkani-Tur, Larry Heck. WHAT IS LEFT TO BE UNDERSTOOD IN ATIS? (2010). https://www.microsoft.com/en-us/research/wp-content/uploads/2010/12/SLT10.pdf
Video 3:

Yun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur, Jianfeng Gao, and Li Deng. End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding (2016). https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_ContextualSLU.pdf
Video 4:

Jason P.C. Chiu, Eric Nichols. Named Entity Recognition with Bidirectional LSTM-CNNs (2016). https://arxiv.org/pdf/1511.08308v4.pdf
Video 5:

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao, Asli Celikyilmaz. Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems (2017). https://arxiv.org/pdf/1703.07055.pdf
Matthew Henderson, Blaise Thomson and Jason Williams. Dialog State Tracking Challenge 2 & 3 (2013). http://camdial.org/~mh521/dstc/downloads/handbook.pdf
Nikola Mrksic, Diarmuid O Seaghdha, Tsung-Hsien Wen, Blaise Thomson, Steve Young. Neural Belief Tracker: Data-Driven Dialogue State Tracking (2017). https://arxiv.org/pdf/1606.03777.pdf
Layla El Asri, et al. FRAMES: A CORPUS FOR ADDING MEMORY TOGOAL-ORIENTED DIALOGUE SYSTEMS (2017). https://arxiv.org/pdf/1704.00057.pdf
Video 6:

Xuesong Yang, Yun-Nung Chen, Dilek Hakkani-Tur, Paul Crook, Xiujun Li, Jianfeng Gao, Li Deng. END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER (2017). https://arxiv.org/pdf/1612.00913.pdf




Congratulations for finishing with the lectures in our course!

NLP is a huge and rapidly emerging area. So to have an up-to-date understanding of its advances one should always keep track of what is going. In these reading material we provide some links for you that give a nice overview of NLP trends as for the end of 2017.

First, it is always a good idea to check out highlights from main conferences. There are nicely summarized trends of ACL-2017: part 1, part 2. Also, some highlights from EMNLP-2017 are available here. Second, it would be a good idea to monitor some blogs, e.g. Sebastian Ruder has nice posts about DL in NLP, optimization trends, word embeddings, and many others.

One of still active topics is Thought Vectors and how one can interpret directions in the hidden space. E.g. you might be interested to check out this post. However, it's getting more clear that compressing all the input into one vector is often not enough and one might make nice things with attention and linguistic information. Some more tips about attention here.

Finally, this is another nice overview of 2017 trends in NLP research - advances in unsupervised machine translation seem especially exciting!

To conclude, we would like to say thank you for taking our course and wish best of luck in your future NLP projects!